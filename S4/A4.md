# Assignment 4 EVA7


##  Part 1

### Neural Network Architecture
![image](https://user-images.githubusercontent.com/47341316/137848643-5f99d303-7d1f-4007-b922-2ed3fa105416.png)

																					
![image](https://user-images.githubusercontent.com/47341316/137849009-badc09ed-426b-4321-9602-45b1338cf90d.png)


## Forward Pass Equations
Input propagation in feedword pass happens in the following way.

* The neurons in hidden layer 1 are weighted sum of input neurons. \
``` h1 = w1*i1 + w2*i2  ```
```   h2 = w3*i1 + w4*i2 ```
* Sigmoid Activation Function is applied on h1 and h2 to make our model non-linear.\
```a_h1 = σ(h1) = 1/(1 + exp(-h1)) ```
```a_h2 = σ(h2) = 1/(1 + exp(-h2)) ```
* Neurons in output layer is the weighted sum of activated neurons in hidden layer 1.\
``` o1 = w5*a_h1 + w6*a_h2 ```
 ``` o2 = w7*a_h1 + w8*a_h2  ```
* Sigmoid Activation Function is applied on o1 and o2 to make model non-linear. \
``` a_o1 = σ(o1) = 1/(1 + exp(-o1))```
```a_o2 = σ(o2) = 1/(1 + exp(-o2)) ```
* E1 and E2 are errors/loss of output o1 and o2 respectively. \
``` E1 = 1/2 * (t1 - a_o1)2	```
```E2 = 1/2 * (t2 - a_o2)2	```
* E_Total is the sum of errors/loss of 2 outputs. \
``` E_total = E1 + E2 ```

###  Backpropagation Equations
We need to do perform backpropagation of loss/error to weights, so that we can update the weights for better predictions.It can be done ,by differentiating total error with respective weights and updating the existing weights with calculated gradients.

So, to propagate the loss from total error to all weights, we will need partial derivative of each variable which is present in any possible path from weight to total error.

Lets propagate error 
```
δE_t/δw5 = δ(E1 + E2)/δw5 = δE1/δw5 = δE1/δa_o1 * δa_o1/δo1 * δo1/δw5								
δE1/δa_o1 = δ(1/2 * (t1 - a_o1)^2) / δa_o1 = (t1 - a_o1)*(-1) = a_o1 - t1 								
δa_o1/δo1 = δ(σ(o1))/δo1 = σ(o1) * (1 - σ(o1)) = a_o1 * (1 - a_o1)								
δo1/δw5 = a_h1								
								
δE_t/δw5 = (a_o1 - t1) * a_o1 * (1 - a_o1) * a_h1								
δE_t/δw6 = (a_o1 - t1) * a_o1 * (1 - a_o1) * a_h2								
δE_t/δw7 = (a_o2 - t2) * a_o2 * (1 - a_o2) * a_h1								
δE_t/δw8 = (a_o2 - t2) * a_o2 * (1 - a_o2) * a_h2								
								
δE1/δa_h1 = (a_o1 - t1) * a_o1 * (1 - a_o1) * w5								
δE2/δa_h1 = (a_o2 - t2) * a_o2 * (1 - a_o2) * w7								
δE_t/δa_h1 = δE1/δa_h1 + δE2/δa_h1 								
								
δE_t/δw1 = δE_t/ δa_h1 *  δa_h1/ δh1 *  δh1/ δw1  =  δE_t/ δa_h1 * a_h1 * (1 - a_h1) * i1								
δE_t/δw2 = δE_t/ δa_h1 *  δa_h1/ δh1 *  δh1/ δw2 =  δE_t/ δa_h1 * a_h1 * (1 - a_h1) * i2								
δE_t/δw3 = δE_t/ δa_h2 *  δa_h2/ δh2 *  δh2/ δw3 =  δE_t/ δa_h2 * a_h2 * (1 - a_h2) * i1								
δE_t/δw4 = δE_t/ δa_h2 *  δa_h2/ δh2 *  δh2/ δw4 =  δE_t/ δa_h2 * a_h2 * (1 - a_h2) * i2


```
### Learning rates

![i1](https://user-images.githubusercontent.com/47341316/137852213-cf9f6f86-0efb-41e8-96e3-c1786b4d0b73.PNG)
![i2](https://user-images.githubusercontent.com/47341316/137852313-a95e6159-7e3d-489c-986b-fa2829e2693e.PNG)
![i3](https://user-images.githubusercontent.com/47341316/137852332-922a38fe-6b29-4d54-9726-74374c0aa69f.PNG)
![i4](https://user-images.githubusercontent.com/47341316/137852347-0d95e7a6-2fc3-4f5a-b8e0-b5e078808069.PNG)
![i5](https://user-images.githubusercontent.com/47341316/137852359-e9a8daac-ace9-460a-899b-2acd2ab71fad.PNG)



##  Part 2

### Model
* The CNN Pipeline consists of 3 CNN layers each of 16, 32 and 50 channels respectively. 
* Each channel has a BatchNormalization,Dropout and MaxPooling step to it. 

* Kernel size selected is 3x3 and padding is set to 1 to preserve the resolution of the channels at each CNN layer. 
* After each layer, batch norm followed by a dropout of 5% is applied.
* The resolution is reduced using MaxPooling after each CNN layer.
* The activation function used is ReLU.
* Our last layer is the Fully Connected layer 
* The output of the CNN Pipeline is fed to a fully connected layer of 10 nuerons followed by a Dropout step.

* Training has been done on GPU
* Learning rate lr=0.01 
* Momentum = 0.9
* Batch size of 32, 64, 128, 256 were experimented. 

* As per the problem statement, the total number of parameters of the network is 19,848 (< 20k) and the number of epochs trained is 19 (< 20 epochs)
* The final accuracy (at the end of 19th epoch ) is 99.28 % 

### Model Summary

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 16, 28, 28]             144
              ReLU-2           [-1, 16, 28, 28]               0
       BatchNorm2d-3           [-1, 16, 28, 28]              32
           Dropout-4           [-1, 16, 28, 28]               0
         MaxPool2d-5           [-1, 16, 14, 14]               0
            Conv2d-6           [-1, 32, 14, 14]           4,608
              ReLU-7           [-1, 32, 14, 14]               0
       BatchNorm2d-8           [-1, 32, 14, 14]              64
           Dropout-9           [-1, 32, 14, 14]               0
        MaxPool2d-10             [-1, 32, 3, 3]               0
           Conv2d-11             [-1, 50, 3, 3]          14,400
             ReLU-12             [-1, 50, 3, 3]               0
      BatchNorm2d-13             [-1, 50, 3, 3]             100
          Dropout-14             [-1, 50, 3, 3]               0
        MaxPool2d-15             [-1, 50, 1, 1]               0
           Linear-16                   [-1, 10]             500
================================================================
Total params: 19,848
Trainable params: 19,848
Non-trainable params: 0
----------------------------------------------------------------
```

### Training Logs
```
Epoch Number =  1
loss=0.062293101102113724 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 22.11it/s]

Test set: Average loss: 0.0492, Accuracy: 9847/10000 (98%)

Epoch Number =  2
loss=0.040206607431173325 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.78it/s]

Test set: Average loss: 0.0356, Accuracy: 9886/10000 (99%)

Epoch Number =  3
loss=0.08588200807571411 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.85it/s]

Test set: Average loss: 0.0344, Accuracy: 9895/10000 (99%)

Epoch Number =  4
loss=0.0371597521007061 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.82it/s]

Test set: Average loss: 0.0321, Accuracy: 9907/10000 (99%)

Epoch Number =  5
loss=0.003993797581642866 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.96it/s]

Test set: Average loss: 0.0300, Accuracy: 9906/10000 (99%)

Epoch Number =  6
loss=0.08285342901945114 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.72it/s]

Test set: Average loss: 0.0332, Accuracy: 9896/10000 (99%)

Epoch Number =  7
loss=0.0075829848647117615 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.88it/s]

Test set: Average loss: 0.0265, Accuracy: 9914/10000 (99%)

Epoch Number =  8
loss=0.01992647349834442 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.62it/s]

Test set: Average loss: 0.0297, Accuracy: 9913/10000 (99%)

Epoch Number =  9
loss=0.004099078942090273 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.75it/s]

Test set: Average loss: 0.0290, Accuracy: 9915/10000 (99%)

Epoch Number =  10
loss=0.019100770354270935 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.72it/s]

Test set: Average loss: 0.0265, Accuracy: 9912/10000 (99%)

Epoch Number =  11
loss=0.029913706704974174 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.76it/s]

Test set: Average loss: 0.0260, Accuracy: 9916/10000 (99%)

Epoch Number =  12
loss=0.0032055082265287638 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.69it/s]

Test set: Average loss: 0.0251, Accuracy: 9928/10000 (99%)

Epoch Number =  13
loss=0.018713297322392464 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.67it/s]

Test set: Average loss: 0.0271, Accuracy: 9913/10000 (99%)

Epoch Number =  14
loss=0.019832104444503784 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.62it/s]

Test set: Average loss: 0.0276, Accuracy: 9920/10000 (99%)

Epoch Number =  15
loss=0.019311852753162384 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.87it/s]

Test set: Average loss: 0.0259, Accuracy: 9914/10000 (99%)

Epoch Number =  16
loss=0.008246871642768383 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.85it/s]

Test set: Average loss: 0.0285, Accuracy: 9903/10000 (99%)

Epoch Number =  17
loss=0.020872652530670166 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 22.22it/s]

Test set: Average loss: 0.0278, Accuracy: 9920/10000 (99%)

Epoch Number =  18
loss=0.001436802209354937 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.98it/s]

Test set: Average loss: 0.0256, Accuracy: 9924/10000 (99%)

Epoch Number =  19
loss=0.004901103209704161 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.54it/s]

Test set: Average loss: 0.0275, Accuracy: 9928/10000 (99%)

```

